{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text=\"\"\"\n",
    "PREFACE\n",
    "\n",
    "\n",
    "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
    "for suspecting that all philosophers, in so far as they have been\n",
    "dogmatists, have failed to understand women--that the terrible\n",
    "seriousness and clumsy importunity with which they have usually paid\n",
    "their addresses to Truth, have been unskilled and unseemly methods for\n",
    "winning a woman? Certainly she has never allowed herself to be won; and\n",
    "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
    "indeed, it stands at all! For there are scoffers who maintain that it\n",
    "has fallen, that all dogma lies on the ground--nay more, that it is at\n",
    "its last gasp. But to speak seriously, there are good grounds for hoping\n",
    "that all dogmatizing in philosophy, whatever solemn, whatever conclusive\n",
    "and decided airs it has assumed, may have been only a noble puerilism\n",
    "and tyronism; and probably the time is at hand when it will be once\n",
    "and again understood WHAT has actually sufficed for the basis of such\n",
    "imposing and absolute philosophical edifices as the dogmatists have\n",
    "hitherto reared: perhaps some popular superstition of immemorial time\n",
    "(such as the soul-superstition, which, in the form of subject- and\n",
    "ego-superstition, has not yet ceased doing mischief): perhaps some\n",
    "play upon words, a deception on the part of grammar, or an\n",
    "audacious generalization of very restricted, very personal, very\n",
    "human--all-too-human facts. The philosophy of the dogmatists, it is to\n",
    "be hoped, was only a promise for thousands of years afterwards, as was\n",
    "astrology in still earlier times, in the service of which probably more\n",
    "labour, gold, acuteness, and patience have been spent than on any\n",
    "actual science hitherto: we owe to it, and to its \"super-terrestrial\"\n",
    "pretensions in Asia and Egypt, the grand style of architecture. It seems\n",
    "that in order to inscribe themselves upon the heart of humanity with\n",
    "everlasting claims, all great things have first to wander about the\n",
    "earth as enormous and awe-inspiring caricatures: dogmatic philosophy has\n",
    "been a caricature of this kind--for instance, the Vedanta doctrine in\n",
    "Asia, and Platonism in Europe. Let us not be ungrateful to it, although\n",
    "it must certainly be confessed that the worst, the most tiresome,\n",
    "and the most dangerous of errors hitherto has been a dogmatist\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself to be won; and\\nat present every kind of dogma stands with sad and discouraged mien--IF,\\nindeed, it stands at all! For there are scoffers who maintain that it\\nhas fallen, that all dogma lies on the ground--nay more, that it is at\\nits last gasp. But to speak seriously, there are good grounds for hoping\\nthat all dogmatizing in philosophy, whatever solemn, whatever conclusive\\nand decided airs it has assumed, may have been only a noble puerilism\\nand tyronism; and probably the time is at hand when it will be once\\nand again understood WHAT has actually sufficed for the basis of such\\nimposing and absolute philosophical edifices as the dogmatists have\\nhitherto reared: perhaps some popular superstition of immemorial time\\n(such as the soul-superstition, which, in the form of subject- and\\nego-superstition, has not yet ceased doing mischief): perhaps some\\nplay upon words, a deception on the part of grammar, or an\\naudacious generalization of very restricted, very personal, very\\nhuman--all-too-human facts. The philosophy of the dogmatists, it is to\\nbe hoped, was only a promise for thousands of years afterwards, as was\\nastrology in still earlier times, in the service of which probably more\\nlabour, gold, acuteness, and patience have been spent than on any\\nactual science hitherto: we owe to it, and to its \"super-terrestrial\"\\npretensions in Asia and Egypt, the grand style of architecture. It seems\\nthat in order to inscribe themselves upon the heart of humanity with\\neverlasting claims, all great things have first to wander about the\\nearth as enormous and awe-inspiring caricatures: dogmatic philosophy has\\nbeen a caricature of this kind--for instance, the Vedanta doctrine in\\nAsia, and Platonism in Europe. Let us not be ungrateful to it, although\\nit must certainly be confessed that the worst, the most tiresome,\\nand the most dangerous of errors hitherto has been a dogmatist\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 742\n",
      "Unique characters: 54\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'A': 12, 'B': 13, 'C': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'L': 20, 'N': 21, 'O': 22, 'P': 23, 'R': 24, 'S': 25, 'T': 26, 'U': 27, 'V': 28, 'W': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'r': 46, 's': 47, 't': 48, 'u': 49, 'v': 50, 'w': 51, 'y': 52, 'z': 53}\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing of text\n",
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "print(char_indices)\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khan\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "742/742 [==============================] - 5s 7ms/step - loss: 3.5365\n",
      "Epoch 2/10\n",
      "742/742 [==============================] - 3s 4ms/step - loss: 3.0557\n",
      "Epoch 3/10\n",
      "742/742 [==============================] - 3s 3ms/step - loss: 2.9523\n",
      "Epoch 4/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 2.9385\n",
      "Epoch 5/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 2.8410\n",
      "Epoch 6/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 2.7389\n",
      "Epoch 7/10\n",
      "742/742 [==============================] - 3s 3ms/step - loss: 2.6195\n",
      "Epoch 8/10\n",
      "742/742 [==============================] - 3s 4ms/step - loss: 2.4496\n",
      "Epoch 9/10\n",
      "742/742 [==============================] - 3s 4ms/step - loss: 2.3862\n",
      "Epoch 10/10\n",
      "742/742 [==============================] - 3s 3ms/step - loss: 2.2289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x113594d32b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \", in the form of subject- and\n",
      "ego-superstition, has not yet \"\n"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ temperature: 0.1\n",
      ", in the form of subject- and\n",
      "ego-superstition, has not yet the tor tos to ind and the tor tor tos tor tor to tor tor tor tor to tos tor tor tor to to s as the to tor tor to tor tos tor as tor tor to tos tor tor tor tos and tor tor tos tor tor tod tos tor tor to tor to tor as and and tor tos tor tos tor to to s as the s as the tor tos tor tor to tor to tor as tor tor to t as tor as tor as tor as tor and tor tor to tor as tor as tor to tor as and tor to tor"
     ]
    }
   ],
   "source": [
    "temperature = 0.1\n",
    "print('------ temperature:', temperature)\n",
    "sys.stdout.write(generated_text)\n",
    "for i in range(400):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(generated_text):\n",
    "        sampled[0, t, char_indices[char]] = 1.\n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = chars[next_index]\n",
    "    generated_text += next_char\n",
    "    generated_text = generated_text[1:]\n",
    "    sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
